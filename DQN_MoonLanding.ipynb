{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "Implement DQN for LunarLander\n",
        "\n",
        "This lab is a modified verstion of a notebook from the Deep RL Course on HuggingFace.\n",
        "\n",
        "In this notebook, we'll train a **Deep Q-Network (DQN) agent** to play an Atari game. The agent controls a spaceship, the Lunar Lander, to learn how to **land correctly on the Moon**.\n",
        "\n",
        "\n",
        "### The environment\n",
        "\n",
        "We will use the [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment from Gymnasium. This environment is a classic rocket trajectory optimization problem. According to Pontryaginâ€™s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF46MwbZD00b"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## Install dependencies and create a virtual screen ðŸ”½\n",
        "\n",
        "The first step is to install the dependencies, weâ€™ll install multiple ones.\n",
        "\n",
        "- `gymnasium[box2d]`: Contains the LunarLander-v2 environment\n",
        "- `stable-baselines3[extra]`: The deep reinforcement learning library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "outputs": [],
      "source": [
        "!apt install swig cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AyE_pdxqONj"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3==2.0.0a5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKeXQJsQCYm"
      },
      "source": [
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install virtual screen libraries and create and run a virtual screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5f2cGkdP-mb"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      },
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYvkbef7XEMi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpVFqyENVf"
      },
      "source": [
        "## Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cygWLPGsEQ0m"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrKGGSlENZB"
      },
      "source": [
        "## Create the LunarLander environment and understand how it works\n",
        "\n",
        "### [The environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "The goal is to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep):\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQxZeJ7qONl"
      },
      "source": [
        "#### Vectorized Environment\n",
        "\n",
        "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poFG0uyBqONl"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrE86r5E5IK"
      },
      "source": [
        "## Create the Model\n",
        "\n",
        "Remember the goal: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Based on this, let's build the algorithm we're going to use to solve this Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4yiUM_9_Ka"
      },
      "source": [
        "To solve this problem, we're going to implement DQN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Neural network model for Q-Learning\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Replay Buffer to store experience tuples\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
        "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
        "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
        "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)\n",
        "        dones = torch.FloatTensor([e.done for e in experiences]).to(device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=1e-4)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(buffer_size=200000, batch_size=128)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99  # discount factor\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.05  # minimum exploration rate\n",
        "        self.epsilon_decay = 0.999  # decay rate for epsilon\n",
        "        self.target_update_frequency = 1000  # how often to update the target network\n",
        "        self.update_count = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in replay buffer and learn every few steps\"\"\"\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every time step if we have enough samples in the memory\n",
        "        if len(self.memory) > self.memory.batch_size:\n",
        "            self.learn()\n",
        "\n",
        "        # Update target network periodically\n",
        "        self.update_count += 1\n",
        "        if self.update_count % self.target_update_frequency == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action_values = self.qnetwork_local(state)\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Sample a batch from the replay buffer and update the Q-Network\"\"\"\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
        "\n",
        "        # Get max predicted Q values (for next states) from target model\n",
        "        next_q_values = self.qnetwork_target(next_states).detach().max(1)[0]\n",
        "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # Get the Q values for the actions that were actually taken\n",
        "        current_q_values = self.qnetwork_local(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = nn.SmoothL1Loss()(current_q_values, target_q_values)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update the target network to match the local Q-network\"\"\"\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay the exploration rate\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## Train the DQN agent\n",
        "Let's train our agent for 1,000,000 timesteps, don't forget to use GPU (on your local installation, Google Colab or similar). You will notice that experiments will take considerably longer than previous labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNHHMKNHRSDv"
      },
      "outputs": [],
      "source": [
        "#check for available GPU\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poBCy9u_csyR"
      },
      "outputs": [],
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training the DQN Agent\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "agent = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0)\n",
        "\n",
        "num_episodes = 2000\n",
        "max_timesteps = 500\n",
        "\n",
        "rewards_list = []\n",
        "\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(max_timesteps):\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "\n",
        "    # Decay epsilon after each episode\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    print(f\"Episode {episode}/{num_episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "    # Save the model every 100 episodes\n",
        "    if episode % 100 == 0:\n",
        "        torch.save(agent.qnetwork_local.state_dict(), f\"dqn_lunarlander_{episode}.pth\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WenJlHnaqnfy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import linregress\n",
        "\n",
        "#rewards_list = np.random.normal(loc=0, scale=1, size=500).cumsum()\n",
        "#episodes = np.arange(len(rewards_list))\n",
        "#slope, intercept, r_value, p_value, std_err = linregress(episodes, rewards_list)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rewards_list, label='Total Reward per Episode', color='blue')\n",
        "# Anpassen der Plot-Grenzen und Erstellen von Platz fÃ¼r den Text unterhalb des Plots\n",
        "plt.subplots_adjust(bottom=0.3)  # VergrÃ¶ÃŸert den unteren Rand des Plotbereichs\n",
        "\n",
        "# Platzierung des Textes unterhalb des Plots\n",
        "plt.figtext(0.5, 0.01,\n",
        "            f'Gamma = {agent.gamma}\\n'\n",
        "            f'Epsilon = {agent.epsilon}\\n'\n",
        "            f'Epsilon_min = {agent.epsilon_min}\\n'\n",
        "            f'Epsilon_decay = {agent.epsilon_decay}\\n'\n",
        "            f'Target Update Frequency = {agent.target_update_frequency}\\n'\n",
        "            f'Update Count = {agent.update_count}',\n",
        "            horizontalalignment='center', verticalalignment='bottom', fontsize=10)\n",
        "\n",
        "print(agent.gamma)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Learning Curve of the DQN-Agent')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "## Evaluate the agent\n",
        "- Now that our Lunar Lander agent is trained, we need to **check its performance**.\n",
        "\n",
        "**Note**: When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpno0glsADy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the agent\n",
        "eval_env = gym.make(\"LunarLander-v2\")\n",
        "eval_env.reset()\n",
        "\n",
        "num_episodes = 10\n",
        "rewards = []\n",
        "agent.qnetwork_local.load_state_dict(torch.load('dqn_lunarlander_2000.pth'))\n",
        "\n",
        "#Evaluate the model with 10 evaluation episodes\n",
        "for _ in range(num_episodes):\n",
        "    state = eval_env.reset()\n",
        "    episode_reward = 0\n",
        "    while True:\n",
        "        # Greedy action selection for evaluation\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action_values = agent.qnetwork_local(state_tensor)\n",
        "        action = np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "        # Step the environment\n",
        "        state, reward, done, _ = eval_env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(episode_reward)\n",
        "\n",
        "# Calculate mean and standard deviation of rewards\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "# Print the results\n",
        "print(f\"{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "eval_env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU6O5mVXA2Ti"
      },
      "outputs": [],
      "source": [
        "# Evaluation with video creating and download\n",
        "\n",
        "# Import necessary libraries\n",
        "import gym\n",
        "from gym.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Create a new evaluation environment with video recording\n",
        "video_path = \"./videos\"  # Directory to store the video\n",
        "eval_env = gym.make(\"LunarLander-v2\")\n",
        "eval_env = RecordVideo(eval_env, video_path, episode_trigger=lambda episode_id: episode_id == 0)  # Record only the first episode\n",
        "\n",
        "num_episodes = 10\n",
        "rewards = []\n",
        "agent.qnetwork_local.load_state_dict(torch.load('dqn_lunarlander_2000.pth'))\n",
        "\n",
        "# Evaluate the model with 10 evaluation episodes\n",
        "for episode in range(num_episodes):\n",
        "    state = eval_env.reset()\n",
        "    episode_reward = 0\n",
        "    while True:\n",
        "        # Greedy action selection for evaluation\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action_values = agent.qnetwork_local(state_tensor)\n",
        "        action = np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "        # Step the environment\n",
        "        state, reward, done, _ = eval_env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(episode_reward)\n",
        "    print(f\"Episode {episode + 1} - Reward: {episode_reward:.2f}\")\n",
        "\n",
        "# Calculate mean and standard deviation of rewards\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "eval_env.close()\n",
        "\n",
        "# Notify where the video is saved\n",
        "print(f\"Video saved at: {video_path}\")\n",
        "\n",
        "# Download the video file (assuming Colab)\n",
        "from google.colab import files\n",
        "files.download(f\"{video_path}/rl-video-episode-0.mp4\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
